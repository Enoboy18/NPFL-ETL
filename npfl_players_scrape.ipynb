{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71febfe1",
   "metadata": {},
   "source": [
    "### NPFL Players Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70ee74",
   "metadata": {},
   "source": [
    "### Scraping Abia Warriors Team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "854cdf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Abia_Warriors_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if len(tables) >= 2:\n",
    "    def extract_table_data(table):\n",
    "        headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\")[1:]: \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if len(row_data) == len(headers):\n",
    "                rows.append(row_data)\n",
    "        return pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "\n",
    "    df1 = extract_table_data(tables[0])\n",
    "    df2 = extract_table_data(tables[1])\n",
    "\n",
    "else:\n",
    "    print(\"Tables not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c9aff",
   "metadata": {},
   "source": [
    "### Saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69bddba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to player data.csv\n"
     ]
    }
   ],
   "source": [
    "if len(tables) >= 2:\n",
    "    df1['Team'] = \"Abia Warriors\"\n",
    "    df2['Team'] = \"Abia Warriors\"\n",
    "\n",
    "    df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df_combined.to_csv(\"player_data.csv\", index=False)\n",
    "    print(\"Data saved to player data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27670d9e",
   "metadata": {},
   "source": [
    "### Scraping Akwa United Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53e49a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Akwa_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Akwa United\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2580ff38",
   "metadata": {},
   "source": [
    "### No data on Bayelsa United Squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fa5ce",
   "metadata": {},
   "source": [
    "### No data on Bendel Insurance Squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0b3d1",
   "metadata": {},
   "source": [
    "### Scraping El-Kanemi Warriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69e342e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/El-Kanemi_Warriors_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"El-Kanemi Warriors\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cfcc4",
   "metadata": {},
   "source": [
    "### Scraping Enugu Rangers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11ea30f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Rangers_International_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Enugu Rangers\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39308d94",
   "metadata": {},
   "source": [
    "### Scraping Enyimba F.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b553567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Enyimba_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Enyimba\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef43e3",
   "metadata": {},
   "source": [
    "### No data on Heartland Squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de40a16",
   "metadata": {},
   "source": [
    "### Scraping Ikorodu City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f43a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Ikorodu_City_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Ikorodu City\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626715a",
   "metadata": {},
   "source": [
    "### Scraping Kano Pillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4c08bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Kano_Pillars_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Kano Pillars\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3c43b",
   "metadata": {},
   "source": [
    "### Scraping Kastina United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46462b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Katsina_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Kastina United\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd6a8f",
   "metadata": {},
   "source": [
    "### Scraping Kwara United - No data on Kwara United "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0e99dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Kwara_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Kwara United\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534a07a",
   "metadata": {},
   "source": [
    "### Scraping Lobi Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45093f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Lobi_Stars_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Lobi Stars\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e10c8d",
   "metadata": {},
   "source": [
    "### Scraping Nasawara United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9aafc44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Nasarawa_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Nasawara\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243129b",
   "metadata": {},
   "source": [
    "### Scraping Niger Tornadoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e8cc995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Niger_Tornadoes_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Niger Tornadoes\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b41d86",
   "metadata": {},
   "source": [
    "### Scraping Plateau United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96e8213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Plateau_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Plateau United\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa3c4a",
   "metadata": {},
   "source": [
    "### Scraping Remo Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06428ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Remo_Stars_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Remo Stars\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scraping Rivers United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6e6a384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Rivers_United_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Rivers United\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b50a4",
   "metadata": {},
   "source": [
    "### No data on Shooting Stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a711af1",
   "metadata": {},
   "source": [
    "### Scraping Sunshine Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7d8a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Sunshine_Stars_F.C.\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable football-squad nogrid\")\n",
    "\n",
    "\n",
    "if tables:\n",
    "    all_rows = []\n",
    "    headers = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not headers:\n",
    "            headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    "        \n",
    "        for row in table.find_all(\"tr\")[1:]:  \n",
    "            cells = row.find_all(\"td\")\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            if len(row_data) == len(headers):\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "        df['Team'] = \"Sunshine Stars\"\n",
    "        \n",
    "        # Appending to Player Data\n",
    "        df.to_csv(\"player_data.csv\", mode='a', header=False, index=False)\n",
    "        print(\"Data appended\")\n",
    "    else:\n",
    "        print(\"No rows found.\")\n",
    "else:\n",
    "    print(\"Tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a33e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ac106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
